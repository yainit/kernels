diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1236732..f30a166 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5867,62 +5867,67 @@ build_overlap_sched_groups(struct sched_domain *sd, int cpu)
 	struct sd_data *sdd = sd->private;
 	struct sched_domain *sibling;
 	int i;
+	int tries;
 
 	cpumask_clear(covered);
 
-	for_each_cpu(i, span) {
-		struct cpumask *sg_span;
-
-		if (cpumask_test_cpu(i, covered))
-			continue;
-
-		sibling = *per_cpu_ptr(sdd->sd, i);
-
-		/* See the comment near build_group_mask(). */
-		if (!cpumask_test_cpu(i, sched_domain_span(sibling)))
-			continue;
-
-		sg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),
-				GFP_KERNEL, cpu_to_node(cpu));
-
-		if (!sg)
-			goto fail;
-
-		sg_span = sched_group_cpus(sg);
-		if (sibling->child)
-			cpumask_copy(sg_span, sched_domain_span(sibling->child));
-		else
-			cpumask_set_cpu(i, sg_span);
-
-		cpumask_or(covered, covered, sg_span);
-
-		sg->sgc = *per_cpu_ptr(sdd->sgc, i);
-		if (atomic_inc_return(&sg->sgc->ref) == 1)
-			build_group_mask(sd, sg);
-
-		/*
-		 * Initialize sgc->capacity such that even if we mess up the
-		 * domains and no possible iteration will get us here, we won't
-		 * die on a /0 trap.
-		 */
-		sg->sgc->capacity = SCHED_CAPACITY_SCALE * cpumask_weight(sg_span);
-
-		/*
-		 * Make sure the first group of this domain contains the
-		 * canonical balance cpu. Otherwise the sched_domain iteration
-		 * breaks. See update_sg_lb_stats().
-		 */
-		if ((!groups && cpumask_test_cpu(cpu, sg_span)) ||
-		    group_balance_cpu(sg) == cpu)
-			groups = sg;
-
-		if (!first)
-			first = sg;
-		if (last)
-			last->next = sg;
-		last = sg;
-		last->next = first;
-	}
+   for(tries = 0; tries < 2; tries++) {
+      for_each_cpu(i, span) {
+         struct cpumask *sg_span;
+         if(tries == 0 && i != cpu)
+            continue;
+
+         if (cpumask_test_cpu(i, covered))
+            continue;
+
+         sibling = *per_cpu_ptr(sdd->sd, i);
+
+         /* See the comment near build_group_mask(). */
+         if (!cpumask_test_cpu(i, sched_domain_span(sibling)))
+            continue;
+
+         sg = kzalloc_node(sizeof(struct sched_group) + cpumask_size(),
+               GFP_KERNEL, cpu_to_node(cpu));
+
+         if (!sg)
+            goto fail;
+
+         sg_span = sched_group_cpus(sg);
+         if (sibling->child)
+            cpumask_copy(sg_span, sched_domain_span(sibling->child));
+         else
+            cpumask_set_cpu(i, sg_span);
+
+         cpumask_or(covered, covered, sg_span);
+
+         sg->sgc = *per_cpu_ptr(sdd->sgc, i);
+         if (atomic_inc_return(&sg->sgc->ref) == 1)
+            build_group_mask(sd, sg);
+
+         /*
+          * Initialize sgc->capacity such that even if we mess up the
+          * domains and no possible iteration will get us here, we won't
+          * die on a /0 trap.
+          */
+         sg->sgc->capacity = SCHED_CAPACITY_SCALE * cpumask_weight(sg_span);
+
+         /*
+          * Make sure the first group of this domain contains the
+          * canonical balance cpu. Otherwise the sched_domain iteration
+          * breaks. See update_sg_lb_stats().
+          */
+         if ((!groups && cpumask_test_cpu(cpu, sg_span)) ||
+               group_balance_cpu(sg) == cpu)
+            groups = sg;
+
+         if (!first)
+            first = sg;
+         if (last)
+            last->next = sg;
+         last = sg;
+         last->next = first;
+      }
+   }
 	sd->groups = groups;
 
 	return 0;
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c2980e8..979d733 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -7654,36 +7601,7 @@
 
 static int should_we_balance(struct lb_env *env)
 {
-	struct sched_group *sg = env->sd->groups;
-	struct cpumask *sg_cpus, *sg_mask;
-	int cpu, balance_cpu = -1;
-
-	/*
-	 * In the newly idle case, we will allow all the cpu's
-	 * to do the newly idle load balance.
-	 */
-	if (env->idle == CPU_NEWLY_IDLE)
-		return 1;
-
-	sg_cpus = sched_group_cpus(sg);
-	sg_mask = sched_group_mask(sg);
-	/* Try to find first idle cpu */
-	for_each_cpu_and(cpu, sg_cpus, env->cpus) {
-		if (!cpumask_test_cpu(cpu, sg_mask) || !idle_cpu(cpu))
-			continue;
-
-		balance_cpu = cpu;
-		break;
-	}
-
-	if (balance_cpu == -1)
-		balance_cpu = group_balance_cpu(sg);
-
-	/*
-	 * First idle cpu or the first cpu(busiest) in this sched group
-	 * is eligible for doing load balancing at this and above domains.
-	 */
-	return balance_cpu == env->dst_cpu;
+	return 1;
 }
 
 /*
