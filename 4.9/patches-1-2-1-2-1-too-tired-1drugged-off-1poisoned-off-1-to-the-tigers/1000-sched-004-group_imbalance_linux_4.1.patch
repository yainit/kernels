diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c2980e8..979d733 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -6759,6 +6759,8 @@
 	unsigned int sum_nr_running; /* Nr tasks running in the group */
 	unsigned int idle_cpus;
 	unsigned int group_weight;
+	unsigned long min_load;
+	unsigned long max_load;
 	enum group_type group_type;
 	int group_no_capacity;
 #ifdef CONFIG_NUMA_BALANCING
@@ -7058,6 +7060,8 @@
 			bool *overload)
 {
 	unsigned long load;
+	unsigned long min_load = ULONG_MAX;
+	unsigned long max_load = 0UL;
 	int i, nr_running;
 
 	memset(sgs, 0, sizeof(*sgs));
@@ -7071,6 +7075,11 @@
 		else
 			load = source_load(i, load_idx);
 
+		if (load < min_load)
+			min_load = load;
+		if(load > max_load)
+			max_load = load;
+
 		sgs->group_load += load;
 		sgs->group_util += cpu_util(i);
 		sgs->sum_nr_running += rq->cfs.h_nr_running;
@@ -7095,6 +7104,9 @@
 	sgs->group_capacity = group->sgc->capacity;
 	sgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;
 
+	sgs->min_load = min_load;
+	sgs->max_load = max_load;
+
 	if (sgs->sum_nr_running)
 		sgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;
 
@@ -7124,18 +7136,16 @@
 {
 	struct sg_lb_stats *busiest = &sds->busiest_stat;
 
-	if (sgs->group_type > busiest->group_type)
-		return true;
-
-	if (sgs->group_type < busiest->group_type)
-		return false;
-
-	if (sgs->avg_load <= busiest->avg_load)
-		return false;
-
+	if (sgs->min_load <= busiest->min_load)
+	   return true;
+	
+	if (sgs->group_type == group_imbalanced)
+	   return true;
+	
 	/* This is the busiest node in its class. */
 	if (!(env->sd->flags & SD_ASYM_PACKING))
 		return true;
+	}
 
 	/* No ASYM_PACKING if target cpu is already busy */
 	if (env->idle == CPU_NOT_IDLE)
@@ -7396,65 +7406,14 @@
 
 	local = &sds->local_stat;
 	busiest = &sds->busiest_stat;
-
-	if (busiest->group_type == group_imbalanced) {
-		/*
-		 * In the group_imb case we cannot rely on group-wide averages
-		 * to ensure cpu-load equilibrium, look at wider averages. XXX
-		 */
-		busiest->load_per_task =
-			min(busiest->load_per_task, sds->avg_load);
-	}
-
-	/*
-	 * Avg load of busiest sg can be less and avg load of local sg can
-	 * be greater than avg load across all sgs of sd because avg load
-	 * factors in sg capacity and sgs with smaller group_type are
-	 * skipped when updating the busiest sg:
-	 */
-	if (busiest->avg_load <= sds->avg_load ||
-	    local->avg_load >= sds->avg_load) {
+	if(local->min_load >= busiest->max_load) {
 		env->imbalance = 0;
-		return fix_small_imbalance(env, sds);
 	}
-
-	/*
-	 * If there aren't any idle cpus, avoid creating some.
-	 */
-	if (busiest->group_type == group_overloaded &&
-	    local->group_type   == group_overloaded) {
-		load_above_capacity = busiest->sum_nr_running * SCHED_CAPACITY_SCALE;
-		if (load_above_capacity > busiest->group_capacity) {
-			load_above_capacity -= busiest->group_capacity;
-			load_above_capacity *= scale_load_down(NICE_0_LOAD);
-			load_above_capacity /= busiest->group_capacity;
-		} else
-			load_above_capacity = ~0UL;
+	else {
+		env->imbalance = (busiest->max_load - local->min_load) / 2;
 	}
+	return;
 
-	/*
-	 * We're trying to get all the cpus to the average_load, so we don't
-	 * want to push ourselves above the average load, nor do we wish to
-	 * reduce the max loaded cpu below the average load. At the same time,
-	 * we also don't want to reduce the group load below the group
-	 * capacity. Thus we look for the minimum possible imbalance.
-	 */
-	max_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);
-
-	/* How much load to actually move to equalise the imbalance */
-	env->imbalance = min(
-		max_pull * busiest->group_capacity,
-		(sds->avg_load - local->avg_load) * local->group_capacity
-	) / SCHED_CAPACITY_SCALE;
-
-	/*
-	 * if *imbalance is less than the average load per runnable task
-	 * there is no guarantee that any tasks will be moved so we'll have
-	 * a think about bumping its value to force at least one task to be
-	 * moved
-	 */
-	if (env->imbalance < busiest->load_per_task)
-		return fix_small_imbalance(env, sds);
 }
 
 /******* find_busiest_group() helpers end here *********************/
@@ -7509,39 +7468,13 @@
 	    busiest->group_no_capacity)
 		goto force_balance;
 
-	/*
-	 * If the local group is busier than the selected busiest group
-	 * don't try and pull any tasks.
-	 */
-	if (local->avg_load >= busiest->avg_load)
-		goto out_balanced;
-
-	/*
-	 * Don't pull any tasks if this group is already above the domain
-	 * average load.
-	 */
-	if (local->avg_load >= sds.avg_load)
+	if(local->min_load < busiest->min_load)
+	{
+		goto force_balance;
+	}
+	else
+	{
 		goto out_balanced;
-
-	if (env->idle == CPU_IDLE) {
-		/*
-		 * This cpu is idle. If the busiest group is not overloaded
-		 * and there is no imbalance between this and busiest group
-		 * wrt idle cpus, it is balanced. The imbalance becomes
-		 * significant if the diff is greater than 1 otherwise we
-		 * might end up to just move the imbalance on another group
-		 */
-		if ((busiest->group_type != group_overloaded) &&
-				(local->idle_cpus <= (busiest->idle_cpus + 1)))
-			goto out_balanced;
-	} else {
-		/*
-		 * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use
-		 * imbalance_pct to be conservative.
-		 */
-		if (100 * busiest->avg_load <=
-				env->sd->imbalance_pct * local->avg_load)
-			goto out_balanced;
 	}
 
 force_balance:
